{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn Exercise   AI240 - 10459\n",
    "## Instructor - Dr. Uma Gajendragadkar\n",
    "\n",
    "## Exercise 1 - \n",
    "First install Numpy Library (if it's not already installed) as it is required by Scikit Learn Library.\n",
    "\n",
    "!pip install numpy\n",
    "\n",
    "If already not installed, then  install Scikit Learn Library using below instruction on Jupyter Notebook\n",
    "\n",
    "!pip install scikit-learn\n",
    "\n",
    "Once the installation is complete, you can verify whether Scikit-learn is installed or not by typing the following command:\n",
    "\n",
    "pip show scikit-learn\n",
    "\n",
    "## Exercise 2 - \n",
    "First import pandas and numpy with below statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "Import the scikit Learn Library datasets with below statement\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "## Exercise 3 - \n",
    "Load default datsets - the iris dataset with below statetments.\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "The Iris dataset, a classic in machine learning, is a dataset of 150 iris flower measurements, including sepal and petal length and width, used for classification tasks, and is readily available within the scikit-learn library.\n",
    "\n",
    "A dataset is a dictionary-like object that holds all the data and some metadata about the data.The variables of data are called its features. They are also known as predictors or attributes.\n",
    "\n",
    "Feature matrix − It is the collection of features, in case there are more than one.\n",
    "\n",
    "Feature Names − list of names of the features.\n",
    "\n",
    "Response − It is the output variable that basically depends upon the feature variables. They are also known as target, label or output.\n",
    "\n",
    "Response Vector − It is used to represent response column. Generally, we have just one response column.\n",
    "\n",
    "Target Names − It represent the possible values taken by a response vector.\n",
    "\n",
    "## Exercise 4 - \n",
    "Lets's use IRIS dataset and execute below lines of code and check the output.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "\n",
    "y = iris.target\n",
    "\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Feature names:\", feature_names)\n",
    "\n",
    "print(\"Target names:\", target_names)\n",
    "\n",
    "print(\"\\nFirst 10 rows of X:\\n\", X[:10])\n",
    "\n",
    "\n",
    "## Exercise 5 - \n",
    "Splitting the dataset\n",
    "Split the dataset into two pieces-a training set and a testing set. Use the training set to train the model and testing set to test the model. Execute below lines of code to split the data into 80:20 ratio . It means 80% data is training data and 20% as testing data.\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "print(y_test.shape)\n",
    "\n",
    "## Exercise 6 - \n",
    "Train the Model\n",
    "Use our dataset to train some prediction-model. Scikit-learn has wide range of Machine Learning (ML) algorithms.\n",
    "\n",
    "Here, we are going to use KNN (K nearest neighbors) classifier for sake of learning the scikit Library. We are going to learn classification in detail in coming weeks. Below lines of code are to be executed and are used to make you understand the implementation. \n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "classifier_knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "classifier_knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier_knn.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Numpy Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\derrick.harden\\appdata\\local\\anaconda3\\lib\\site-packages (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\derrick.harden\\appdata\\local\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\derrick.harden\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\derrick.harden\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\derrick.harden\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\derrick.harden\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify installation of Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-learn\n",
      "Version: 1.6.1\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: https://scikit-learn.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: BSD 3-Clause License\n",
      "\n",
      " Copyright (c) 2007-2024 The scikit-learn developers.\n",
      " All rights reserved.\n",
      "\n",
      " Redistribution and use in source and binary forms, with or without\n",
      " modification, are permitted provided that the following conditions are met:\n",
      "\n",
      " * Redistributions of source code must retain the above copyright notice, this\n",
      "   list of conditions and the following disclaimer.\n",
      "\n",
      " * Redistributions in binary form must reproduce the above copyright notice,\n",
      "   this list of conditions and the following disclaimer in the documentation\n",
      "   and/or other materials provided with the distribution.\n",
      "\n",
      " * Neither the name of the copyright holder nor the names of its\n",
      "   contributors may be used to endorse or promote products derived from\n",
      "   this software without specific prior written permission.\n",
      "\n",
      " THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      " AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      " IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      " DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      " FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      " DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      " SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      " CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      " OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      " OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "\n",
      "Location: C:\\Users\\derrick.harden\\AppData\\Local\\anaconda3\\Lib\\site-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: imbalanced-learn, sklearn-compat\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Default Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the IRIS Dataset to execute code and check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target_names: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "First 10 rows of X\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "# Print \n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Target_names:\", target_names)\n",
    "print(\"\\nFirst 10 rows of X\\n\", X[:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Excercise 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into two pieces - a training set and a testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into 80:20 ratio. 80% training data and 20% testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(30, 4)\n",
      "(120,)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Excercise 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign\n",
    "classifier_knn = KNeighborsClassifier(n_neighbors = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier_knn.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
